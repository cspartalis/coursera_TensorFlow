{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kaml/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRnDnCW-Z7qv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n",
      "263\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soPGVheskaQP"
   },
   "outputs": [],
   "source": [
    "\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJtwVB2NbOAP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "66\n",
      "8\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index['in'])\n",
    "print(tokenizer.word_index['the'])\n",
    "print(tokenizer.word_index['town'])\n",
    "print(tokenizer.word_index['of'])\n",
    "print(tokenizer.word_index['athy'])\n",
    "print(tokenizer.word_index['one'])\n",
    "print(tokenizer.word_index['jeremy'])\n",
    "print(tokenizer.word_index['lanigan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49Cv68JOakwv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  4  2 66  8 67 68 69]\n"
     ]
    }
   ],
   "source": [
    "print(xs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iY-jwvfgbEF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(ys[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtzlUMYadhKt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  4  2 66  8 67 68]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(xs[5])\n",
    "print(ys[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4myRpB1c4Gg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9vH8Y59ajYL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kaml/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 453 samples\n",
      "Epoch 1/500\n",
      "453/453 [==============================] - 1s 2ms/sample - loss: 5.5665 - accuracy: 0.0397\n",
      "Epoch 2/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 5.5352 - accuracy: 0.0508\n",
      "Epoch 3/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 5.4552 - accuracy: 0.0508\n",
      "Epoch 4/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 5.2629 - accuracy: 0.0508\n",
      "Epoch 5/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 5.1167 - accuracy: 0.0508\n",
      "Epoch 6/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 5.0602 - accuracy: 0.0508\n",
      "Epoch 7/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 5.0247 - accuracy: 0.0508\n",
      "Epoch 8/500\n",
      "453/453 [==============================] - 0s 184us/sample - loss: 4.9927 - accuracy: 0.0508\n",
      "Epoch 9/500\n",
      "453/453 [==============================] - 0s 154us/sample - loss: 4.9607 - accuracy: 0.0530\n",
      "Epoch 10/500\n",
      "453/453 [==============================] - 0s 163us/sample - loss: 4.9186 - accuracy: 0.0552\n",
      "Epoch 11/500\n",
      "453/453 [==============================] - 0s 155us/sample - loss: 4.8733 - accuracy: 0.0574\n",
      "Epoch 12/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 4.8219 - accuracy: 0.0662\n",
      "Epoch 13/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 4.7653 - accuracy: 0.0662\n",
      "Epoch 14/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 4.7133 - accuracy: 0.0861\n",
      "Epoch 15/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 4.6581 - accuracy: 0.0905\n",
      "Epoch 16/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 4.6098 - accuracy: 0.0861\n",
      "Epoch 17/500\n",
      "453/453 [==============================] - 0s 152us/sample - loss: 4.5643 - accuracy: 0.0883\n",
      "Epoch 18/500\n",
      "453/453 [==============================] - 0s 156us/sample - loss: 4.5209 - accuracy: 0.0883\n",
      "Epoch 19/500\n",
      "453/453 [==============================] - 0s 152us/sample - loss: 4.4850 - accuracy: 0.0927\n",
      "Epoch 20/500\n",
      "453/453 [==============================] - 0s 158us/sample - loss: 4.4444 - accuracy: 0.0949\n",
      "Epoch 21/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 4.4106 - accuracy: 0.0927\n",
      "Epoch 22/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 4.3697 - accuracy: 0.1082\n",
      "Epoch 23/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 4.3291 - accuracy: 0.1148\n",
      "Epoch 24/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 4.2922 - accuracy: 0.1192\n",
      "Epoch 25/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 4.2474 - accuracy: 0.1170\n",
      "Epoch 26/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 4.2090 - accuracy: 0.1369\n",
      "Epoch 27/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 4.1861 - accuracy: 0.1391\n",
      "Epoch 28/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 4.1431 - accuracy: 0.1280\n",
      "Epoch 29/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 4.1058 - accuracy: 0.1302\n",
      "Epoch 30/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 4.0772 - accuracy: 0.1325\n",
      "Epoch 31/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 4.0367 - accuracy: 0.1347\n",
      "Epoch 32/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 4.0005 - accuracy: 0.1457\n",
      "Epoch 33/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 3.9742 - accuracy: 0.1457\n",
      "Epoch 34/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 3.9513 - accuracy: 0.1501\n",
      "Epoch 35/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 3.9196 - accuracy: 0.1611\n",
      "Epoch 36/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 3.8808 - accuracy: 0.1678\n",
      "Epoch 37/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 3.8271 - accuracy: 0.1744\n",
      "Epoch 38/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 3.7894 - accuracy: 0.1854\n",
      "Epoch 39/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 3.7546 - accuracy: 0.1854\n",
      "Epoch 40/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 3.7181 - accuracy: 0.1898\n",
      "Epoch 41/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 3.6827 - accuracy: 0.2009\n",
      "Epoch 42/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 3.6496 - accuracy: 0.2097\n",
      "Epoch 43/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 3.6120 - accuracy: 0.2097\n",
      "Epoch 44/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 3.5693 - accuracy: 0.2274\n",
      "Epoch 45/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 3.5336 - accuracy: 0.2428\n",
      "Epoch 46/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 3.4989 - accuracy: 0.2428\n",
      "Epoch 47/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 3.4653 - accuracy: 0.2494\n",
      "Epoch 48/500\n",
      "453/453 [==============================] - 0s 155us/sample - loss: 3.4307 - accuracy: 0.2517\n",
      "Epoch 49/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 3.3941 - accuracy: 0.2583\n",
      "Epoch 50/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 3.3604 - accuracy: 0.2826\n",
      "Epoch 51/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 3.3307 - accuracy: 0.2892\n",
      "Epoch 52/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 3.3000 - accuracy: 0.2870\n",
      "Epoch 53/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 3.2626 - accuracy: 0.2914\n",
      "Epoch 54/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 3.2297 - accuracy: 0.3024\n",
      "Epoch 55/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 3.2024 - accuracy: 0.3157\n",
      "Epoch 56/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 3.1802 - accuracy: 0.3333\n",
      "Epoch 57/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 3.1496 - accuracy: 0.3289\n",
      "Epoch 58/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 3.1128 - accuracy: 0.3289\n",
      "Epoch 59/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 3.0864 - accuracy: 0.3422\n",
      "Epoch 60/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 3.0466 - accuracy: 0.3532\n",
      "Epoch 61/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 3.0132 - accuracy: 0.3664\n",
      "Epoch 62/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 2.9862 - accuracy: 0.3929\n",
      "Epoch 63/500\n",
      "453/453 [==============================] - 0s 161us/sample - loss: 2.9724 - accuracy: 0.3709\n",
      "Epoch 64/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 2.9400 - accuracy: 0.3819\n",
      "Epoch 65/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 2.8952 - accuracy: 0.3819\n",
      "Epoch 66/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 2.8667 - accuracy: 0.3929\n",
      "Epoch 67/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 2.8415 - accuracy: 0.3929\n",
      "Epoch 68/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 2.8095 - accuracy: 0.4106\n",
      "Epoch 69/500\n",
      "453/453 [==============================] - 0s 163us/sample - loss: 2.7948 - accuracy: 0.4084\n",
      "Epoch 70/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 2.7779 - accuracy: 0.4327\n",
      "Epoch 71/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 2.7355 - accuracy: 0.4481\n",
      "Epoch 72/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 2.7035 - accuracy: 0.4481\n",
      "Epoch 73/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 2.6745 - accuracy: 0.4547\n",
      "Epoch 74/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 2.6467 - accuracy: 0.4680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 2.6180 - accuracy: 0.4680\n",
      "Epoch 76/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 2.5864 - accuracy: 0.4857\n",
      "Epoch 77/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 2.5651 - accuracy: 0.4879\n",
      "Epoch 78/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 2.5389 - accuracy: 0.5055\n",
      "Epoch 79/500\n",
      "453/453 [==============================] - 0s 161us/sample - loss: 2.5081 - accuracy: 0.5143\n",
      "Epoch 80/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 2.4812 - accuracy: 0.5342\n",
      "Epoch 81/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 2.4569 - accuracy: 0.5298\n",
      "Epoch 82/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 2.4315 - accuracy: 0.5364\n",
      "Epoch 83/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 2.4074 - accuracy: 0.5497\n",
      "Epoch 84/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 2.3795 - accuracy: 0.5563\n",
      "Epoch 85/500\n",
      "453/453 [==============================] - 0s 156us/sample - loss: 2.3553 - accuracy: 0.5563\n",
      "Epoch 86/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 2.3345 - accuracy: 0.5673\n",
      "Epoch 87/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 2.3112 - accuracy: 0.5673\n",
      "Epoch 88/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 2.2944 - accuracy: 0.5607\n",
      "Epoch 89/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 2.2720 - accuracy: 0.5717\n",
      "Epoch 90/500\n",
      "453/453 [==============================] - 0s 164us/sample - loss: 2.2490 - accuracy: 0.6004\n",
      "Epoch 91/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 2.2407 - accuracy: 0.5828\n",
      "Epoch 92/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 2.2662 - accuracy: 0.5585\n",
      "Epoch 93/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 2.2420 - accuracy: 0.5541\n",
      "Epoch 94/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 2.2241 - accuracy: 0.5651\n",
      "Epoch 95/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 2.1928 - accuracy: 0.5850\n",
      "Epoch 96/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 2.1769 - accuracy: 0.5916\n",
      "Epoch 97/500\n",
      "453/453 [==============================] - 0s 155us/sample - loss: 2.1681 - accuracy: 0.5850\n",
      "Epoch 98/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 2.1458 - accuracy: 0.6049\n",
      "Epoch 99/500\n",
      "453/453 [==============================] - 0s 154us/sample - loss: 2.1087 - accuracy: 0.6247\n",
      "Epoch 100/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 2.0889 - accuracy: 0.6269\n",
      "Epoch 101/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 2.0538 - accuracy: 0.6468\n",
      "Epoch 102/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 2.0393 - accuracy: 0.6446\n",
      "Epoch 103/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 2.0060 - accuracy: 0.6578\n",
      "Epoch 104/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 1.9863 - accuracy: 0.6468\n",
      "Epoch 105/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.9587 - accuracy: 0.6689\n",
      "Epoch 106/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 1.9353 - accuracy: 0.6755\n",
      "Epoch 107/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.9164 - accuracy: 0.6777\n",
      "Epoch 108/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.8953 - accuracy: 0.6755\n",
      "Epoch 109/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 1.8832 - accuracy: 0.6799\n",
      "Epoch 110/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 1.8641 - accuracy: 0.6887\n",
      "Epoch 111/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 1.8482 - accuracy: 0.6887\n",
      "Epoch 112/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.8377 - accuracy: 0.6932\n",
      "Epoch 113/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 1.8189 - accuracy: 0.7020\n",
      "Epoch 114/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.8011 - accuracy: 0.6998\n",
      "Epoch 115/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 1.7804 - accuracy: 0.7108\n",
      "Epoch 116/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 1.7587 - accuracy: 0.7042\n",
      "Epoch 117/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 1.7426 - accuracy: 0.7130\n",
      "Epoch 118/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 1.7242 - accuracy: 0.7152\n",
      "Epoch 119/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 1.7086 - accuracy: 0.7108\n",
      "Epoch 120/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 1.6991 - accuracy: 0.7108\n",
      "Epoch 121/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.6872 - accuracy: 0.7130\n",
      "Epoch 122/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 1.6730 - accuracy: 0.7196\n",
      "Epoch 123/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 1.6510 - accuracy: 0.7285\n",
      "Epoch 124/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 1.6369 - accuracy: 0.7241\n",
      "Epoch 125/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 1.6193 - accuracy: 0.7196\n",
      "Epoch 126/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 1.6034 - accuracy: 0.7174\n",
      "Epoch 127/500\n",
      "453/453 [==============================] - 0s 157us/sample - loss: 1.5897 - accuracy: 0.7307\n",
      "Epoch 128/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 1.5718 - accuracy: 0.7329\n",
      "Epoch 129/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 1.5555 - accuracy: 0.7439\n",
      "Epoch 130/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 1.5389 - accuracy: 0.7439\n",
      "Epoch 131/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 1.5273 - accuracy: 0.7461\n",
      "Epoch 132/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 1.5355 - accuracy: 0.7417\n",
      "Epoch 133/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 1.5215 - accuracy: 0.7417\n",
      "Epoch 134/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 1.5100 - accuracy: 0.7572\n",
      "Epoch 135/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 1.4929 - accuracy: 0.7572\n",
      "Epoch 136/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 1.4738 - accuracy: 0.7572\n",
      "Epoch 137/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 1.4487 - accuracy: 0.7726\n",
      "Epoch 138/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.4334 - accuracy: 0.7726\n",
      "Epoch 139/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 1.4152 - accuracy: 0.7792\n",
      "Epoch 140/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.4036 - accuracy: 0.7815\n",
      "Epoch 141/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 1.3912 - accuracy: 0.7947\n",
      "Epoch 142/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 1.3804 - accuracy: 0.7925\n",
      "Epoch 143/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.3679 - accuracy: 0.7903\n",
      "Epoch 144/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 1.3567 - accuracy: 0.8013\n",
      "Epoch 145/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 1.3376 - accuracy: 0.7881\n",
      "Epoch 146/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 1.3228 - accuracy: 0.7969\n",
      "Epoch 147/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 1.3089 - accuracy: 0.8124\n",
      "Epoch 148/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 1.2967 - accuracy: 0.8079\n",
      "Epoch 149/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 1.2861 - accuracy: 0.8168\n",
      "Epoch 150/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 1.2769 - accuracy: 0.8146\n",
      "Epoch 151/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 149us/sample - loss: 1.2623 - accuracy: 0.8146\n",
      "Epoch 152/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 1.2490 - accuracy: 0.8102\n",
      "Epoch 153/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 1.2411 - accuracy: 0.8146\n",
      "Epoch 154/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 1.2321 - accuracy: 0.8079\n",
      "Epoch 155/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.2260 - accuracy: 0.8124\n",
      "Epoch 156/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 1.2156 - accuracy: 0.8256\n",
      "Epoch 157/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 1.2193 - accuracy: 0.8146\n",
      "Epoch 158/500\n",
      "453/453 [==============================] - 0s 152us/sample - loss: 1.2015 - accuracy: 0.8234\n",
      "Epoch 159/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 1.2060 - accuracy: 0.8212\n",
      "Epoch 160/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 1.2188 - accuracy: 0.8212\n",
      "Epoch 161/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 1.1895 - accuracy: 0.8278\n",
      "Epoch 162/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 1.1635 - accuracy: 0.8234\n",
      "Epoch 163/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 1.1494 - accuracy: 0.8389\n",
      "Epoch 164/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 1.1359 - accuracy: 0.8411\n",
      "Epoch 165/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 1.1423 - accuracy: 0.8256\n",
      "Epoch 166/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 1.1421 - accuracy: 0.8256\n",
      "Epoch 167/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 1.1123 - accuracy: 0.8499\n",
      "Epoch 168/500\n",
      "453/453 [==============================] - 0s 155us/sample - loss: 1.1230 - accuracy: 0.8389\n",
      "Epoch 169/500\n",
      "453/453 [==============================] - 0s 157us/sample - loss: 1.1315 - accuracy: 0.8389\n",
      "Epoch 170/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 1.1041 - accuracy: 0.8344\n",
      "Epoch 171/500\n",
      "453/453 [==============================] - 0s 154us/sample - loss: 1.0721 - accuracy: 0.8433\n",
      "Epoch 172/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 1.0645 - accuracy: 0.8565\n",
      "Epoch 173/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 1.0467 - accuracy: 0.8521\n",
      "Epoch 174/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 1.0344 - accuracy: 0.8521\n",
      "Epoch 175/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 1.0229 - accuracy: 0.8499\n",
      "Epoch 176/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 1.0556 - accuracy: 0.8322\n",
      "Epoch 177/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 1.0207 - accuracy: 0.8477\n",
      "Epoch 178/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 1.0094 - accuracy: 0.8521\n",
      "Epoch 179/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 1.0045 - accuracy: 0.8455\n",
      "Epoch 180/500\n",
      "453/453 [==============================] - 0s 161us/sample - loss: 0.9881 - accuracy: 0.8521\n",
      "Epoch 181/500\n",
      "453/453 [==============================] - 0s 152us/sample - loss: 0.9838 - accuracy: 0.8587\n",
      "Epoch 182/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 0.9682 - accuracy: 0.8565\n",
      "Epoch 183/500\n",
      "453/453 [==============================] - 0s 154us/sample - loss: 0.9614 - accuracy: 0.8609\n",
      "Epoch 184/500\n",
      "453/453 [==============================] - 0s 152us/sample - loss: 0.9493 - accuracy: 0.8631\n",
      "Epoch 185/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.9357 - accuracy: 0.8631\n",
      "Epoch 186/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.9236 - accuracy: 0.8698\n",
      "Epoch 187/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.9110 - accuracy: 0.8786\n",
      "Epoch 188/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 0.9024 - accuracy: 0.8830\n",
      "Epoch 189/500\n",
      "453/453 [==============================] - 0s 156us/sample - loss: 0.8972 - accuracy: 0.8852\n",
      "Epoch 190/500\n",
      "453/453 [==============================] - 0s 152us/sample - loss: 0.8886 - accuracy: 0.8786\n",
      "Epoch 191/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 0.8771 - accuracy: 0.8786\n",
      "Epoch 192/500\n",
      "453/453 [==============================] - 0s 155us/sample - loss: 0.8681 - accuracy: 0.8830\n",
      "Epoch 193/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.8598 - accuracy: 0.8786\n",
      "Epoch 194/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.8501 - accuracy: 0.8874\n",
      "Epoch 195/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 0.8413 - accuracy: 0.8896\n",
      "Epoch 196/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.8331 - accuracy: 0.8918\n",
      "Epoch 197/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.8263 - accuracy: 0.8874\n",
      "Epoch 198/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.8201 - accuracy: 0.8874\n",
      "Epoch 199/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.8135 - accuracy: 0.8918\n",
      "Epoch 200/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.8051 - accuracy: 0.8918\n",
      "Epoch 201/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.7986 - accuracy: 0.8896\n",
      "Epoch 202/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.7904 - accuracy: 0.8896\n",
      "Epoch 203/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.7830 - accuracy: 0.8985\n",
      "Epoch 204/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.7756 - accuracy: 0.9029\n",
      "Epoch 205/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.7685 - accuracy: 0.8985\n",
      "Epoch 206/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.7615 - accuracy: 0.8985\n",
      "Epoch 207/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.7545 - accuracy: 0.8962\n",
      "Epoch 208/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.7474 - accuracy: 0.9051\n",
      "Epoch 209/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.7422 - accuracy: 0.9007\n",
      "Epoch 210/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.7361 - accuracy: 0.9073\n",
      "Epoch 211/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 0.7290 - accuracy: 0.9095\n",
      "Epoch 212/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.7240 - accuracy: 0.9073\n",
      "Epoch 213/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.7183 - accuracy: 0.9095\n",
      "Epoch 214/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.7101 - accuracy: 0.9095\n",
      "Epoch 215/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 0.7031 - accuracy: 0.9095\n",
      "Epoch 216/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 0.6974 - accuracy: 0.9073\n",
      "Epoch 217/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.6925 - accuracy: 0.9095\n",
      "Epoch 218/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.6858 - accuracy: 0.9095\n",
      "Epoch 219/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 0.6795 - accuracy: 0.9117\n",
      "Epoch 220/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.6736 - accuracy: 0.9161\n",
      "Epoch 221/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.6657 - accuracy: 0.9161\n",
      "Epoch 222/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.6600 - accuracy: 0.9139\n",
      "Epoch 223/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.6542 - accuracy: 0.9183\n",
      "Epoch 224/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.6476 - accuracy: 0.9161\n",
      "Epoch 225/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.6434 - accuracy: 0.9205\n",
      "Epoch 226/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.6381 - accuracy: 0.9139\n",
      "Epoch 227/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 139us/sample - loss: 0.6334 - accuracy: 0.9183\n",
      "Epoch 228/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.6266 - accuracy: 0.9183\n",
      "Epoch 229/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.6214 - accuracy: 0.9205\n",
      "Epoch 230/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.6186 - accuracy: 0.9183\n",
      "Epoch 231/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.6131 - accuracy: 0.9183\n",
      "Epoch 232/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.6086 - accuracy: 0.9205\n",
      "Epoch 233/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.6020 - accuracy: 0.9249\n",
      "Epoch 234/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.5963 - accuracy: 0.9205\n",
      "Epoch 235/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.5931 - accuracy: 0.9227\n",
      "Epoch 236/500\n",
      "453/453 [==============================] - 0s 159us/sample - loss: 0.5899 - accuracy: 0.9183\n",
      "Epoch 237/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.5868 - accuracy: 0.9272\n",
      "Epoch 238/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.5847 - accuracy: 0.9205\n",
      "Epoch 239/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.5865 - accuracy: 0.9183\n",
      "Epoch 240/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.5754 - accuracy: 0.9227\n",
      "Epoch 241/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.5692 - accuracy: 0.9272\n",
      "Epoch 242/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.5626 - accuracy: 0.9294\n",
      "Epoch 243/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.6284 - accuracy: 0.9051\n",
      "Epoch 244/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.6017 - accuracy: 0.9073\n",
      "Epoch 245/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.5864 - accuracy: 0.9139\n",
      "Epoch 246/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.5781 - accuracy: 0.9227\n",
      "Epoch 247/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.5714 - accuracy: 0.9183\n",
      "Epoch 248/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.5825 - accuracy: 0.9073\n",
      "Epoch 249/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.5740 - accuracy: 0.9139\n",
      "Epoch 250/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.5840 - accuracy: 0.9073\n",
      "Epoch 251/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.5721 - accuracy: 0.9161\n",
      "Epoch 252/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 0.5840 - accuracy: 0.9095\n",
      "Epoch 253/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.5752 - accuracy: 0.9073\n",
      "Epoch 254/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.5406 - accuracy: 0.9205\n",
      "Epoch 255/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.5327 - accuracy: 0.9227\n",
      "Epoch 256/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 0.5224 - accuracy: 0.9249\n",
      "Epoch 257/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.5270 - accuracy: 0.9272\n",
      "Epoch 258/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.5154 - accuracy: 0.9272\n",
      "Epoch 259/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 0.5135 - accuracy: 0.9316\n",
      "Epoch 260/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.4950 - accuracy: 0.9382\n",
      "Epoch 261/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.4955 - accuracy: 0.9338\n",
      "Epoch 262/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.4978 - accuracy: 0.9316\n",
      "Epoch 263/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.4948 - accuracy: 0.9272\n",
      "Epoch 264/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.4832 - accuracy: 0.9294\n",
      "Epoch 265/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.4736 - accuracy: 0.9338\n",
      "Epoch 266/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 0.4668 - accuracy: 0.9316\n",
      "Epoch 267/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.4616 - accuracy: 0.9338\n",
      "Epoch 268/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.4575 - accuracy: 0.9360\n",
      "Epoch 269/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.4539 - accuracy: 0.9316\n",
      "Epoch 270/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.4480 - accuracy: 0.9382\n",
      "Epoch 271/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 0.4430 - accuracy: 0.9382\n",
      "Epoch 272/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.4402 - accuracy: 0.9382\n",
      "Epoch 273/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.4359 - accuracy: 0.9404\n",
      "Epoch 274/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.4316 - accuracy: 0.9404\n",
      "Epoch 275/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.4286 - accuracy: 0.9382\n",
      "Epoch 276/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.4256 - accuracy: 0.9426\n",
      "Epoch 277/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 0.4217 - accuracy: 0.9426\n",
      "Epoch 278/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.4183 - accuracy: 0.9426\n",
      "Epoch 279/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.4151 - accuracy: 0.9426\n",
      "Epoch 280/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.4123 - accuracy: 0.9448\n",
      "Epoch 281/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.4081 - accuracy: 0.9404\n",
      "Epoch 282/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 0.4058 - accuracy: 0.9404\n",
      "Epoch 283/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.4021 - accuracy: 0.9448\n",
      "Epoch 284/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.3995 - accuracy: 0.9448\n",
      "Epoch 285/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 0.3961 - accuracy: 0.9404\n",
      "Epoch 286/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.3926 - accuracy: 0.9426\n",
      "Epoch 287/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.3898 - accuracy: 0.9426\n",
      "Epoch 288/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.3870 - accuracy: 0.9404\n",
      "Epoch 289/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.3843 - accuracy: 0.9404\n",
      "Epoch 290/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3811 - accuracy: 0.9426\n",
      "Epoch 291/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3783 - accuracy: 0.9448\n",
      "Epoch 292/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.3754 - accuracy: 0.9404\n",
      "Epoch 293/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3733 - accuracy: 0.9426\n",
      "Epoch 294/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.3699 - accuracy: 0.9382\n",
      "Epoch 295/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 0.3680 - accuracy: 0.9426\n",
      "Epoch 296/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.3675 - accuracy: 0.9404\n",
      "Epoch 297/500\n",
      "453/453 [==============================] - 0s 130us/sample - loss: 0.3632 - accuracy: 0.9382\n",
      "Epoch 298/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.3605 - accuracy: 0.9360\n",
      "Epoch 299/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.3590 - accuracy: 0.9382\n",
      "Epoch 300/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3599 - accuracy: 0.9404\n",
      "Epoch 301/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.3560 - accuracy: 0.9404\n",
      "Epoch 302/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.3518 - accuracy: 0.9426\n",
      "Epoch 303/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 134us/sample - loss: 0.3502 - accuracy: 0.9426\n",
      "Epoch 304/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.3456 - accuracy: 0.9426\n",
      "Epoch 305/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3439 - accuracy: 0.9448\n",
      "Epoch 306/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3427 - accuracy: 0.9448\n",
      "Epoch 307/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.3399 - accuracy: 0.9448\n",
      "Epoch 308/500\n",
      "453/453 [==============================] - 0s 130us/sample - loss: 0.3400 - accuracy: 0.9426\n",
      "Epoch 309/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.3414 - accuracy: 0.9404\n",
      "Epoch 310/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3362 - accuracy: 0.9448\n",
      "Epoch 311/500\n",
      "453/453 [==============================] - 0s 130us/sample - loss: 0.3341 - accuracy: 0.9426\n",
      "Epoch 312/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3301 - accuracy: 0.9426\n",
      "Epoch 313/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3268 - accuracy: 0.9470\n",
      "Epoch 314/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.3237 - accuracy: 0.9448\n",
      "Epoch 315/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.3217 - accuracy: 0.9426\n",
      "Epoch 316/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.3194 - accuracy: 0.9448\n",
      "Epoch 317/500\n",
      "453/453 [==============================] - 0s 130us/sample - loss: 0.3170 - accuracy: 0.9426\n",
      "Epoch 318/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3159 - accuracy: 0.9426\n",
      "Epoch 319/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3142 - accuracy: 0.9404\n",
      "Epoch 320/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.3129 - accuracy: 0.9426\n",
      "Epoch 321/500\n",
      "453/453 [==============================] - 0s 130us/sample - loss: 0.3108 - accuracy: 0.9404\n",
      "Epoch 322/500\n",
      "453/453 [==============================] - 0s 129us/sample - loss: 0.3084 - accuracy: 0.9448\n",
      "Epoch 323/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.3086 - accuracy: 0.9404\n",
      "Epoch 324/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.3031 - accuracy: 0.9404\n",
      "Epoch 325/500\n",
      "453/453 [==============================] - 0s 163us/sample - loss: 0.3009 - accuracy: 0.9426\n",
      "Epoch 326/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.2988 - accuracy: 0.9470\n",
      "Epoch 327/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.2963 - accuracy: 0.9448\n",
      "Epoch 328/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2944 - accuracy: 0.9426\n",
      "Epoch 329/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.2933 - accuracy: 0.9448\n",
      "Epoch 330/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.2920 - accuracy: 0.9426\n",
      "Epoch 331/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.2905 - accuracy: 0.9426\n",
      "Epoch 332/500\n",
      "453/453 [==============================] - 0s 158us/sample - loss: 0.2937 - accuracy: 0.9426\n",
      "Epoch 333/500\n",
      "453/453 [==============================] - 0s 151us/sample - loss: 0.3062 - accuracy: 0.9382\n",
      "Epoch 334/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.3110 - accuracy: 0.9382\n",
      "Epoch 335/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.3117 - accuracy: 0.9382\n",
      "Epoch 336/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.3098 - accuracy: 0.9404\n",
      "Epoch 337/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 0.3145 - accuracy: 0.9316\n",
      "Epoch 338/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.3252 - accuracy: 0.9382\n",
      "Epoch 339/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.3188 - accuracy: 0.9382\n",
      "Epoch 340/500\n",
      "453/453 [==============================] - 0s 155us/sample - loss: 0.3135 - accuracy: 0.9338\n",
      "Epoch 341/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.2955 - accuracy: 0.9382\n",
      "Epoch 342/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.2892 - accuracy: 0.9404\n",
      "Epoch 343/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.2952 - accuracy: 0.9404\n",
      "Epoch 344/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.2869 - accuracy: 0.9404\n",
      "Epoch 345/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.2829 - accuracy: 0.9426\n",
      "Epoch 346/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2780 - accuracy: 0.9514\n",
      "Epoch 347/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 0.2748 - accuracy: 0.9492\n",
      "Epoch 348/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.2719 - accuracy: 0.9404\n",
      "Epoch 349/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 0.2682 - accuracy: 0.9404\n",
      "Epoch 350/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.2672 - accuracy: 0.9426\n",
      "Epoch 351/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.2648 - accuracy: 0.9470\n",
      "Epoch 352/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.2623 - accuracy: 0.9448\n",
      "Epoch 353/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.2605 - accuracy: 0.9448\n",
      "Epoch 354/500\n",
      "453/453 [==============================] - 0s 130us/sample - loss: 0.2599 - accuracy: 0.9448\n",
      "Epoch 355/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2572 - accuracy: 0.9470\n",
      "Epoch 356/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.2581 - accuracy: 0.9470\n",
      "Epoch 357/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.2558 - accuracy: 0.9470\n",
      "Epoch 358/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.2531 - accuracy: 0.9492\n",
      "Epoch 359/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.2527 - accuracy: 0.9448\n",
      "Epoch 360/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.2509 - accuracy: 0.9448\n",
      "Epoch 361/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.2522 - accuracy: 0.9492\n",
      "Epoch 362/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.2487 - accuracy: 0.9470\n",
      "Epoch 363/500\n",
      "453/453 [==============================] - 0s 127us/sample - loss: 0.2465 - accuracy: 0.9426\n",
      "Epoch 364/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.2452 - accuracy: 0.9470\n",
      "Epoch 365/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.2429 - accuracy: 0.9448\n",
      "Epoch 366/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 0.2408 - accuracy: 0.9448\n",
      "Epoch 367/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.2389 - accuracy: 0.9448\n",
      "Epoch 368/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.2371 - accuracy: 0.9448\n",
      "Epoch 369/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.2362 - accuracy: 0.9426\n",
      "Epoch 370/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.2342 - accuracy: 0.9470\n",
      "Epoch 371/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.2323 - accuracy: 0.9470\n",
      "Epoch 372/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.2311 - accuracy: 0.9470\n",
      "Epoch 373/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2303 - accuracy: 0.9470\n",
      "Epoch 374/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.2293 - accuracy: 0.9426\n",
      "Epoch 375/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.2277 - accuracy: 0.9426\n",
      "Epoch 376/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2264 - accuracy: 0.9448\n",
      "Epoch 377/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.2248 - accuracy: 0.9426\n",
      "Epoch 378/500\n",
      "453/453 [==============================] - 0s 154us/sample - loss: 0.2236 - accuracy: 0.9492\n",
      "Epoch 379/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 147us/sample - loss: 0.2224 - accuracy: 0.9492\n",
      "Epoch 380/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.2214 - accuracy: 0.9470\n",
      "Epoch 381/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.2207 - accuracy: 0.9492\n",
      "Epoch 382/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2195 - accuracy: 0.9448\n",
      "Epoch 383/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.2184 - accuracy: 0.9426\n",
      "Epoch 384/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.2172 - accuracy: 0.9448\n",
      "Epoch 385/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.2157 - accuracy: 0.9492\n",
      "Epoch 386/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.2149 - accuracy: 0.9470\n",
      "Epoch 387/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.2191 - accuracy: 0.9448\n",
      "Epoch 388/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.2166 - accuracy: 0.9492\n",
      "Epoch 389/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.2128 - accuracy: 0.9536\n",
      "Epoch 390/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2158 - accuracy: 0.9448\n",
      "Epoch 391/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2156 - accuracy: 0.9492\n",
      "Epoch 392/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.2142 - accuracy: 0.9492\n",
      "Epoch 393/500\n",
      "453/453 [==============================] - 0s 130us/sample - loss: 0.2118 - accuracy: 0.9470\n",
      "Epoch 394/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.2111 - accuracy: 0.9492\n",
      "Epoch 395/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.2083 - accuracy: 0.9470\n",
      "Epoch 396/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.2057 - accuracy: 0.9470\n",
      "Epoch 397/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.2039 - accuracy: 0.9470\n",
      "Epoch 398/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.2035 - accuracy: 0.9448\n",
      "Epoch 399/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.2016 - accuracy: 0.9470\n",
      "Epoch 400/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.2011 - accuracy: 0.9448\n",
      "Epoch 401/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.2005 - accuracy: 0.9448\n",
      "Epoch 402/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.1996 - accuracy: 0.9448\n",
      "Epoch 403/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.1984 - accuracy: 0.9448\n",
      "Epoch 404/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.1980 - accuracy: 0.9448\n",
      "Epoch 405/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.1966 - accuracy: 0.9448\n",
      "Epoch 406/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 0.1957 - accuracy: 0.9448\n",
      "Epoch 407/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.1955 - accuracy: 0.9404\n",
      "Epoch 408/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.1972 - accuracy: 0.9448\n",
      "Epoch 409/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.2021 - accuracy: 0.9426\n",
      "Epoch 410/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 0.1991 - accuracy: 0.9470\n",
      "Epoch 411/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.1965 - accuracy: 0.9448\n",
      "Epoch 412/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.1932 - accuracy: 0.9514\n",
      "Epoch 413/500\n",
      "453/453 [==============================] - 0s 155us/sample - loss: 0.1915 - accuracy: 0.9492\n",
      "Epoch 414/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.1912 - accuracy: 0.9448\n",
      "Epoch 415/500\n",
      "453/453 [==============================] - 0s 152us/sample - loss: 0.1896 - accuracy: 0.9426\n",
      "Epoch 416/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.1883 - accuracy: 0.9426\n",
      "Epoch 417/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 0.1863 - accuracy: 0.9470\n",
      "Epoch 418/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.1870 - accuracy: 0.9448\n",
      "Epoch 419/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.1860 - accuracy: 0.9470\n",
      "Epoch 420/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.1857 - accuracy: 0.9470\n",
      "Epoch 421/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.1851 - accuracy: 0.9470\n",
      "Epoch 422/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.1854 - accuracy: 0.9426\n",
      "Epoch 423/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.1833 - accuracy: 0.9404\n",
      "Epoch 424/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.1814 - accuracy: 0.9448\n",
      "Epoch 425/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 0.1805 - accuracy: 0.9426\n",
      "Epoch 426/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 0.1801 - accuracy: 0.9448\n",
      "Epoch 427/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.1785 - accuracy: 0.9404\n",
      "Epoch 428/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.1785 - accuracy: 0.9470\n",
      "Epoch 429/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.1780 - accuracy: 0.9470\n",
      "Epoch 430/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.1773 - accuracy: 0.9492\n",
      "Epoch 431/500\n",
      "453/453 [==============================] - 0s 154us/sample - loss: 0.1761 - accuracy: 0.9492\n",
      "Epoch 432/500\n",
      "453/453 [==============================] - 0s 139us/sample - loss: 0.1750 - accuracy: 0.9426\n",
      "Epoch 433/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.1741 - accuracy: 0.9492\n",
      "Epoch 434/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.1739 - accuracy: 0.9448\n",
      "Epoch 435/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.1728 - accuracy: 0.9426\n",
      "Epoch 436/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.1718 - accuracy: 0.9470\n",
      "Epoch 437/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.1710 - accuracy: 0.9470\n",
      "Epoch 438/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.1707 - accuracy: 0.9470\n",
      "Epoch 439/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.1703 - accuracy: 0.9470\n",
      "Epoch 440/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.1694 - accuracy: 0.9470\n",
      "Epoch 441/500\n",
      "453/453 [==============================] - 0s 129us/sample - loss: 0.1689 - accuracy: 0.9448\n",
      "Epoch 442/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.1677 - accuracy: 0.9492\n",
      "Epoch 443/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.1673 - accuracy: 0.9448\n",
      "Epoch 444/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.1668 - accuracy: 0.9492\n",
      "Epoch 445/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.1661 - accuracy: 0.9492\n",
      "Epoch 446/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.1655 - accuracy: 0.9426\n",
      "Epoch 447/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.1647 - accuracy: 0.9470\n",
      "Epoch 448/500\n",
      "453/453 [==============================] - 0s 131us/sample - loss: 0.1644 - accuracy: 0.9448\n",
      "Epoch 449/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.1639 - accuracy: 0.9448\n",
      "Epoch 450/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.1637 - accuracy: 0.9470\n",
      "Epoch 451/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.1627 - accuracy: 0.9404\n",
      "Epoch 452/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.1620 - accuracy: 0.9426\n",
      "Epoch 453/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.1631 - accuracy: 0.9382\n",
      "Epoch 454/500\n",
      "453/453 [==============================] - 0s 132us/sample - loss: 0.1634 - accuracy: 0.9492\n",
      "Epoch 455/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 131us/sample - loss: 0.1629 - accuracy: 0.9448\n",
      "Epoch 456/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.1646 - accuracy: 0.9426\n",
      "Epoch 457/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.1687 - accuracy: 0.9426\n",
      "Epoch 458/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.1834 - accuracy: 0.9426\n",
      "Epoch 459/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.2048 - accuracy: 0.9360\n",
      "Epoch 460/500\n",
      "453/453 [==============================] - 0s 149us/sample - loss: 0.1961 - accuracy: 0.9382\n",
      "Epoch 461/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.2264 - accuracy: 0.9205\n",
      "Epoch 462/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.2689 - accuracy: 0.9249\n",
      "Epoch 463/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.2628 - accuracy: 0.9161\n",
      "Epoch 464/500\n",
      "453/453 [==============================] - 0s 153us/sample - loss: 0.2686 - accuracy: 0.9139\n",
      "Epoch 465/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.2769 - accuracy: 0.9227\n",
      "Epoch 466/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 0.2944 - accuracy: 0.9117\n",
      "Epoch 467/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.2442 - accuracy: 0.9272\n",
      "Epoch 468/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2477 - accuracy: 0.9272\n",
      "Epoch 469/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2421 - accuracy: 0.9227\n",
      "Epoch 470/500\n",
      "453/453 [==============================] - 0s 133us/sample - loss: 0.2112 - accuracy: 0.9338\n",
      "Epoch 471/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.2398 - accuracy: 0.9205\n",
      "Epoch 472/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.2027 - accuracy: 0.9338\n",
      "Epoch 473/500\n",
      "453/453 [==============================] - 0s 144us/sample - loss: 0.2281 - accuracy: 0.9316\n",
      "Epoch 474/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 0.2350 - accuracy: 0.9272\n",
      "Epoch 475/500\n",
      "453/453 [==============================] - 0s 155us/sample - loss: 0.1978 - accuracy: 0.9426\n",
      "Epoch 476/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.1786 - accuracy: 0.9426\n",
      "Epoch 477/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.1735 - accuracy: 0.9404\n",
      "Epoch 478/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 0.1704 - accuracy: 0.9404\n",
      "Epoch 479/500\n",
      "453/453 [==============================] - 0s 148us/sample - loss: 0.1683 - accuracy: 0.9404\n",
      "Epoch 480/500\n",
      "453/453 [==============================] - 0s 146us/sample - loss: 0.1660 - accuracy: 0.9404\n",
      "Epoch 481/500\n",
      "453/453 [==============================] - 0s 152us/sample - loss: 0.1643 - accuracy: 0.9470\n",
      "Epoch 482/500\n",
      "453/453 [==============================] - 0s 150us/sample - loss: 0.1628 - accuracy: 0.9492\n",
      "Epoch 483/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.1617 - accuracy: 0.9448\n",
      "Epoch 484/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.1614 - accuracy: 0.9448\n",
      "Epoch 485/500\n",
      "453/453 [==============================] - 0s 141us/sample - loss: 0.1605 - accuracy: 0.9426\n",
      "Epoch 486/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.1596 - accuracy: 0.9492\n",
      "Epoch 487/500\n",
      "453/453 [==============================] - 0s 145us/sample - loss: 0.1591 - accuracy: 0.9492\n",
      "Epoch 488/500\n",
      "453/453 [==============================] - 0s 135us/sample - loss: 0.1603 - accuracy: 0.9470\n",
      "Epoch 489/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.1580 - accuracy: 0.9470\n",
      "Epoch 490/500\n",
      "453/453 [==============================] - 0s 143us/sample - loss: 0.1570 - accuracy: 0.9426\n",
      "Epoch 491/500\n",
      "453/453 [==============================] - 0s 142us/sample - loss: 0.1561 - accuracy: 0.9448\n",
      "Epoch 492/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.1553 - accuracy: 0.9448\n",
      "Epoch 493/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.1546 - accuracy: 0.9426\n",
      "Epoch 494/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.1533 - accuracy: 0.9426\n",
      "Epoch 495/500\n",
      "453/453 [==============================] - 0s 140us/sample - loss: 0.1536 - accuracy: 0.9492\n",
      "Epoch 496/500\n",
      "453/453 [==============================] - 0s 138us/sample - loss: 0.1522 - accuracy: 0.9514\n",
      "Epoch 497/500\n",
      "453/453 [==============================] - 0s 136us/sample - loss: 0.1518 - accuracy: 0.9470\n",
      "Epoch 498/500\n",
      "453/453 [==============================] - 0s 147us/sample - loss: 0.1504 - accuracy: 0.9514\n",
      "Epoch 499/500\n",
      "453/453 [==============================] - 0s 137us/sample - loss: 0.1500 - accuracy: 0.9492\n",
      "Epoch 500/500\n",
      "453/453 [==============================] - 0s 134us/sample - loss: 0.1489 - accuracy: 0.9492\n"
     ]
    }
   ],
   "source": [
    "  model = Sequential()\n",
    "  model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "  model.add(Bidirectional(LSTM(20)))\n",
    "  model.add(Dense(total_words, activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  history = model.fit(xs, ys, epochs=500, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YXGelKThoTT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poeprYK8h-c7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Vc6PHgxa6Hm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laurence went to dublin the nice girls got all entangled nelly gave creature water water saw odaly polkas polkas saw creature saw gave entangled entangled entangled creature glisten with glisten glisten tea father creature eyes nelly steps forget ball didnt didnt didnt them til come as the ask ask ask murther painted meelia as as glisten glisten glisten saw creature saw gave odaly father father call relations relations gave creature murther glisten glisten father creature eyes nelly steps forget ball didnt didnt them til gray and the ructions of kerrigan all nonsensical polkas polkas polkas ladies jeremy end for nelly twas hearty hearty up\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Laurence went to dublin\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Course 3 - Week 4 - Lesson 1 - Notebook.ipynb",
   "provenance": [
    {
     "file_id": "1V60jn23JMcMpwhF-KvCTYIIfm4J2X6v5",
     "timestamp": 1558707866173
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
